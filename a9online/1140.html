<!DOCTYPE html>
<html>
<head profile="http://www.w3.org/2005/10/profile">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="static/reset.css" />
  <link rel="stylesheet" type="text/css" href="static/viewer-style.css" />
  <link rel="stylesheet" type="text/css" href="static/content-style.css" />
  <link rel="stylesheet" type="text/css" href="static/print-style.css" media="print" />
  <script type="text/javascript" src="static/mathjax/MathJax.js?config=local"></script>
  <title>LSA / PLSA / LDA</title>
  <link type="image/x-icon" rel="shortcut icon" href="static/icons/favicon-pad.png">
</head>

<body>
  <div id="wrapper">
    <div id="content-frame">
      <div id="content-name" title="LSA / PLSA / LDA">LSA / PLSA / LDA</div>
      <div id="content">
<p><strong>LSA</strong>, <strong>PLSA</strong>, and <strong>LDA</strong> are methods for modeling semantics of words based on topics.</p>
<p><strong><em>Main Idea</em></strong> Words with similar meaning will occur in similar documents.</p>
<h1 id="latent-semantic-analysis-lsa-">Latent Semantic Analysis (LSA)</h1>
<p>The <strong>latent</strong> in <strong>Latent Semantic Analysis</strong> (LSA) means latent topics. Basically, LSA finds low-dimension representation of documents and words.</p>
<p>Given documents $d_1, \dots, d_m$ and vocabulary words $w_1, \dots, w_n$, we construct a <strong>document-term matrix</strong> $X \in \RR^{m\times n}$ where $x_{ij}$ describes the occurrence of word $w_j$ in document $d_i$. (For example, $x_{ij}$ can be the raw count, 0-1 count, or TF-IDF.)</p>
<div class="table-wrapper"><table>
<thead>
<tr>
<th></th>
<th>cat</th>
<th>dog</th>
<th>computer</th>
<th>internet</th>
<th>rabbit</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>D1</strong></td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td><strong>D2</strong></td>
<td>3</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td><strong>D3</strong></td>
<td>0</td>
<td>0</td>
<td>3</td>
<td>4</td>
<td>0</td>
</tr>
<tr>
<td><strong>D4</strong></td>
<td>5</td>
<td>0</td>
<td>2</td>
<td>5</td>
<td>0</td>
</tr>
<tr>
<td><strong>D5</strong></td>
<td>2</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table></div>
<p>The dot product of row vectors is the document similarity, while the dot product of column vectors is the word similarity.</p>
<p>To reduce the dimensionality of $X$, apply truncated SVD:
$$X \approx U_t\Sigma_t V_t\trps$$
Each column of $U_t$ ($\in \RR^{m\times t}$) and $V_t$ ($\in \RR^{n\times t}$) corresponds to a document <strong>topic</strong>. Now we can:</p>
<ul>
<li>find similarity between $w_i$ and $w_j$ by finding the dot product of rows $i$ and $j$ of $V_t$.</li>
<li>find documents relevant to the search query $d^*$ by applying the SVD mapping on $d^*$ and taking dot products with the rows of $U_t$.</li>
</ul>
<h1 id="probabilistic-latent-semantic-analysis-plsa-">Probabilistic Latent Semantic Analysis (PLSA)</h1>
<p>Instead of using matrices, <strong>Probabilistic Latent Semantic Analysis</strong> (PLSA) uses a probabilistic method. Its graphical model is</p>
<svg fill="white" stroke="black" width="230" height="120">
<g transform="translate(10,10)"><rect fill="white" height="100" width="210" y="0" x="0"></rect><foreignObject height="100" width="210" y="0" x="0"><div style="display: table-cell; width: 210px; height: 94px; text-align: right; vertical-align: bottom; padding: 3px;">$M$</div></foreignObject></g>
<g transform="translate(80,20)"><rect fill="white" height="60" width="130" y="0" x="0"></rect><foreignObject height="60" width="130" y="0" x="0"><div style="display: table-cell; width: 130px; height: 54px; text-align: right; vertical-align: bottom; padding: 3px;">$N$</div></foreignObject></g>
<g transform="translate(45,50)"><circle fill="lightgray" r="16"></circle><foreignObject height="32" width="32" y="-16" x="-16"><div style="line-height: 32px; text-align: center;">$d$</div></foreignObject></g>
<g transform="translate(115,50)"><circle fill="white" r="16"></circle><foreignObject height="32" width="32" y="-16" x="-16"><div style="line-height: 32px; text-align: center;">$c$</div></foreignObject></g>
<g transform="translate(175,50)"><circle fill="lightgray" r="16"></circle><foreignObject height="32" width="32" y="-16" x="-16"><div style="line-height: 32px; text-align: center;">$w$</div></foreignObject></g>
<g><line stroke="black" y2="50" x2="99" y1="50" x1="61"></line><polygon transform="translate(99,50) rotate(0)" fill="black" stroke="none" points="0,0 -6,3 -6,-3"></polygon></g> <g><line stroke="black" y2="50" x2="159" y1="50" x1="131"></line><polygon transform="translate(159,50) rotate(0)" fill="black" stroke="none" points="0,0 -6,3 -6,-3"></polygon></g>
</svg>

<p>$$\prob{w\mid d} = \prob{d}\sum_c \prob{c\mid d}\prob{w\mid c}$$</p>
<ul>
<li>$d$ = document index</li>
<li>$c$ = word's topic drawn from $\prob{c \mid d}$</li>
<li>$w$ = word drawn from $\prob{w \mid c}$</li>
</ul>
<p>Both $\prob{c\mid d}$ and $\prob{w\mid c}$ are modeled as multinomial distributions. The parameters can be trained with EM or whatever.</p>
<p>One pitfall is the lack of parameters for $\prob{d}$, so we don't know how to assign probability to a new document. Another is that the number of parameters for $\prob{c\mid d}$ grows linearly with the number of documents, which leads to overfitting.</p>
<h1 id="latent-dirichlet-allocation-lda-">Latent Dirichlet Allocation (LDA)</h1>
<p><strong><em>Note</em></strong> This is <em>NOT</em> the same as <strong>Linear Discriminant Analysis</strong> (<strong>LDA</strong>). Can't people avoid abbreviation collisions?</p>
<p>We generalize PLSA by changing the fixed $d$ to a Dirichlet prior.</p>
<svg fill="white" stroke="black" width="280" height="180">
<g transform="translate(50,65)">
<g transform="translate(10,10)"><rect fill="white" height="100" width="210" y="0" x="0"></rect><foreignObject height="100" width="210" y="0" x="0"><div style="display: table-cell; width: 210px; height: 94px; text-align: right; vertical-align: bottom; padding: 3px;">$M$</div></foreignObject></g>
<g transform="translate(80,20)"><rect fill="white" height="60" width="130" y="0" x="0"></rect><foreignObject height="60" width="130" y="0" x="0"><div style="display: table-cell; width: 130px; height: 54px; text-align: right; vertical-align: bottom; padding: 3px;">$N$</div></foreignObject></g>
<g transform="translate(45,50)"><circle fill="white" r="16"></circle><foreignObject height="32" width="32" y="-16" x="-16"><div style="line-height: 32px; text-align: center;">$\theta$</div></foreignObject></g>
<g transform="translate(115,50)"><circle fill="white" r="16"></circle><foreignObject height="32" width="32" y="-16" x="-16"><div style="line-height: 32px; text-align: center;">$c$</div></foreignObject></g>
<g transform="translate(175,50)"><circle fill="lightgray" r="16"></circle><foreignObject height="32" width="32" y="-16" x="-16"><div style="line-height: 32px; text-align: center;">$w$</div></foreignObject></g>
<g><line stroke="black" y2="50" x2="99" y1="50" x1="61"></line><polygon transform="translate(99,50) rotate(0)" fill="black" stroke="none" points="0,0 -6,3 -6,-3"></polygon></g> <g><line stroke="black" y2="50" x2="159" y1="50" x1="131"></line><polygon transform="translate(159,50) rotate(0)" fill="black" stroke="none" points="0,0 -6,3 -6,-3"></polygon></g>
</g>
<g transform="translate(35,115)"><rect fill="lightgray" height="28" width="28" y="-14" x="-14"></rect><foreignObject height="28" width="28" y="-14" x="-14"><div style="line-height: 28px; text-align: center;">$\alpha$</div></foreignObject></g> <g><line stroke="black" y2="115" x2="79" y1="115" x1="49"></line><polygon transform="translate(79,115) rotate(0)" fill="black" stroke="none" points="0,0 -6,3 -6,-3"></polygon></g>
<g transform="translate(195,5)"><rect fill="white" height="60" width="70" y="0" x="0"></rect><foreignObject height="60" width="70" y="0" x="0"><div style="display: table-cell; width: 70px; height: 54px; text-align: right; vertical-align: bottom; padding: 3px;">$K$</div></foreignObject></g>
<g transform="translate(225,35)"><circle fill="white" r="16"></circle><foreignObject height="32" width="32" y="-16" x="-16"><div style="line-height: 32px; text-align: center;">$\phi$</div></foreignObject></g> <g><line stroke="black" y2="99" x2="225" y1="51" x1="225"></line><polygon transform="translate(225,99) rotate(90)" fill="black" stroke="none" points="0,0 -6,3 -6,-3"></polygon></g>
<g transform="translate(165,35)"><rect fill="lightgray" height="28" width="28" y="-14" x="-14"></rect><foreignObject height="28" width="28" y="-14" x="-14"><div style="line-height: 28px; text-align: center;">$\beta$</div></foreignObject></g> <g><line stroke="black" y2="35" x2="209" y1="35" x1="179"></line><polygon transform="translate(209,35) rotate(0)" fill="black" stroke="none" points="0,0 -6,3 -6,-3"></polygon></g>
</svg>

<p>The generative process for each word $w_j$ (from a vocab of size $V$) in document $d_i$ is as follow:</p>
<ol>
<li>Choose $\theta_i \sim \Mr{Dir}(\alpha)$ (where $i = 1,\dots,M$; $\theta_i \in \Delta_K$)<ul>
<li>$\theta_{i,k}$ = probability that document $i \in \set{1,\dots,M}$ has topic $k \in \set{1,\dots,K}$.</li>
</ul>
</li>
<li>Choose $\phi_k \sim \Mr{Dir}(\beta)$ (where $k = 1,\dots,K$; $\phi_k \in \Delta_V$)<ul>
<li>$\phi_{k,v}$ = probability of word $v \in \set{1,\dots,V}$ in topic $k \in \set{1,\dots,K}$.</li>
</ul>
</li>
<li>Choose $c_{i,j} \sim \Mr{Polynomial}(\theta_i)$ (where $c_{i,j} \in \set{1,\dots,K}$)</li>
<li>Choose $w_{i,j} \sim \Mr{Polynomial}(\phi_{c_{i,j}})$ (where $w_{i,j} \in \set{1,\dots,V}$)</li>
</ol>
<h1 id="reference">Reference</h1>
<ul>
<li><a target="_blank" href="http://en.wikipedia.org/wiki/Latent_semantic_analysis">http://en.wikipedia.org/wiki/Latent_semantic_analysis</a></li>
<li><a target="_blank" href="http://www.scholarpedia.org/article/Latent_semantic_analysis">http://www.scholarpedia.org/article/Latent_semantic_analysis</a></li>
<li><a target="_blank" href="http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation</a></li>
<li><a target="_blank" href="http://semanticsearchart.com/researchLSA.html">http://semanticsearchart.com/researchLSA.html</a></li>
<li><a target="_blank" href="http://semanticsearchart.com/researchpLSA.html">http://semanticsearchart.com/researchpLSA.html</a></li>
<li><a target="_blank" href="http://jmlr.org/papers/volume3/blei03a/blei03a.pdf">http://jmlr.org/papers/volume3/blei03a/blei03a.pdf</a></li>
</ul>

      </div>
      <div id="content-time">Exported: 2016-07-13T01:48:37.923867</div>
    </div>
  </div>
</body>

</html>
