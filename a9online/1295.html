<!DOCTYPE html>
<html>
<head profile="http://www.w3.org/2005/10/profile">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="static/reset.css" />
  <link rel="stylesheet" type="text/css" href="static/viewer-style.css" />
  <link rel="stylesheet" type="text/css" href="static/content-style.css" />
  <link rel="stylesheet" type="text/css" href="static/print-style.css" media="print" />
  <script type="text/javascript" src="static/mathjax/MathJax.js?config=local"></script>
  <title>Reinforcement Learning / MDP (CS221)</title>
  <link type="image/x-icon" rel="shortcut icon" href="static/icons/favicon-pad.png">
</head>

<body>
  <div id="wrapper">
    <div id="content-frame">
      <div id="content-name" title="Reinforcement Learning / MDP (CS221)">Reinforcement Learning / MDP (CS221)</div>
      <div id="content">
<h1 id="summary-table">Summary Table</h1>
<div class="table-wrapper"><table>
<thead>
<tr>
<th style="text-align:center">Topic</th>
<th style="text-align:center">Algorithm</th>
<th style="text-align:center">Input</th>
<th style="text-align:center">Output</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">MDP</td>
<td style="text-align:center">Policy Evaluation</td>
<td style="text-align:center">$S$, $A$, $T$, $R$, $\pi$</td>
<td style="text-align:center">$V_\pi$, $Q_\pi$</td>
</tr>
<tr>
<td style="text-align:center">MDP</td>
<td style="text-align:center">Policy Iteration / Value Iteration</td>
<td style="text-align:center">$S$, $A$, $T$, $R$</td>
<td style="text-align:center">$\pi_\Mr{opt}$, $V_\Mr{opt}$, $Q_\Mr{opt}$</td>
</tr>
<tr>
<td style="text-align:center">RL</td>
<td style="text-align:center">Model-free Monte Carlo / SARSA</td>
<td style="text-align:center">$S$, $A$, $\pi$, simulation by $\pi$</td>
<td style="text-align:center">$V_\pi$, $Q_\pi$</td>
</tr>
<tr>
<td style="text-align:center">RL</td>
<td style="text-align:center">Model-based Monte Carlo</td>
<td style="text-align:center">$S$, $A$, simulation by any $\pi$</td>
<td style="text-align:center">$T$, $R$ (becomes MDP â†’ can now find $\pi_\Mr{opt}$, $V_\Mr{opt}$, $Q_\Mr{opt}$)</td>
</tr>
<tr>
<td style="text-align:center">RL</td>
<td style="text-align:center">Q-Learning</td>
<td style="text-align:center">$S$, $A$, simulation by any $\pi$</td>
<td style="text-align:center">$\pi_\Mr{opt}$, $V_\Mr{opt}$, $Q_\Mr{opt}$</td>
</tr>
</tbody>
</table></div>
<h1 id="markov-decision-process">Markov Decision Process</h1>
<h2 id="setup">Setup</h2>
<p><strong><em>Definition</em></strong> A <strong>Markov decision process</strong> (<strong>MDP</strong>) consists of</p>
<ul>
<li>$S$ = states, with start state $s_{\text{start}}\in S$</li>
<li>$A(s)$ = actions from state $s$</li>
<li>$T(s,a,s')$ = probability of $s'$ if take action $a$ in state $s$</li>
<li>$0 \leq \gamma \leq 1$ = discount factor</li>
<li>$R(s,a,s')$ = reward of the transition $(s,a,s')$</li>
<li>$\Mr{IsEnd}(s)$ = whether end of game</li>
</ul>
<p><strong><em>Definition</em></strong> A <strong>policy</strong> $\pi$ is a deterministic map from each state $s\in S$ to an action $a\in A$.</p>
<p><strong><em>Definition</em></strong> The <strong>utility</strong> of a random path generated by $\pi$ is the discounted sum of the rewards: for a path (<strong>episode</strong>)
$$s_0, a_1, r_1, s_1, a_2, r_2, s_2, \dots$$
the utility is
$$u = r_1 + \gamma r_2 + \gamma^2 r_3 + \dots$$
The <strong>value</strong> of $\pi$ is the expected utility.</p>
<p><strong><em>Definition</em></strong> For a given $\pi$,</p>
<ul>
<li>$V_\pi(s)$ (<strong>value</strong>) = expected utility by following $\pi$ from state $s$ (The term "value" is overloaded.)</li>
<li>$Q_\pi(s,a)$ (<strong>Q-value</strong>) = expected utility by taking action $a$ from state $s$ and then following $\pi$</li>
</ul>
<h2 id="policy-evaluation">Policy Evaluation</h2>
<p><strong><em>Goal</em></strong> Given an MDP $(S,A,T,R)$ and a policy $\pi$, compute $V_\pi(\cdot)$</p>
<p><strong><em>Recurrence</em></strong>
$$V_\pi(s) = \cases{
0 &amp; \Mr{IsEnd}(s) \\
Q_\pi(s,\pi(s)) &amp; \cotherw}$$
$$Q_\pi(s,a) = \sum_{s'}T(s,a,s')\crab{R(s,a,s') + \gamma V_\pi(s')}$$</p>
<p><strong><em>Algorithm</em></strong> (<strong>Policy Evaluation</strong>)</p>
<blockquote>
<ul>
<li>Initialize $V_\pi(s) \gets 0$ for all $s$</li>
<li>Until convergence:<blockquote>
<ul>
<li>For each $s \in S$, Update $$V_\pi(s) \gets \sum_{s'}T(s,\pi(s),s')\crab{R(s,\pi(s),s') + \gamma V_\pi(s')}$$ where the term in the sum is just the current estimate of $Q_\pi(s,\pi(s))$.</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<h2 id="policy-optimization">Policy Optimization</h2>
<p><strong><em>Goal</em></strong> Given an MDP $(S,A,T,R)$, find a policy $\pi$ that maximizes the value.</p>
<p>We give 2 algorithms: <strong>Policy Iteration</strong> and <strong>Value Iteration</strong></p>
<p><strong><em>Algorithm</em></strong> (<strong>Policy Iteration</strong>) Update $\pi$ directly.</p>
<blockquote>
<ul>
<li>Initialize $\pi$ arbitrarily</li>
<li>Until convergence:<blockquote>
<ul>
<li>Run <strong>Policy Evaluation</strong> to compute $V_\pi$</li>
<li>Compute $Q_\pi$ from $V_\pi$ using the recurrence</li>
<li>For each $s\in S$, update $$\pi(s) \gets \argmax_{a\in A(s)} Q_\pi(s,a)$$</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<p><strong><em>Algorithm</em></strong> (<strong>Value Iteration</strong>) Optimizes the optimal value $V_\Mr{opt}(s)$, and then derive $\pi_\Mr{opt}$ from $V_\Mr{opt}(s)$.</p>
<blockquote>
<ul>
<li>Initialize $V_\Mr{opt}(s) = 0$ for all $s$</li>
<li>Until convergence:<blockquote>
<ul>
<li>For each state $s$, update $$V_\Mr{opt}(s) = \max_{a\in A(s)} \sum_{s'}T(s,a,s')\crab{R(s,a,s') + \gamma V_\Mr{opt}(s')}$$ where the term inside max is just the current estimate of $Q_\Mr{opt}(s,a)$.</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<p><strong><em>Theorem</em></strong> If either $\gamma &lt; 1$ or MDP graph is acyclic, then both algorithms above will converge to the correct answer. (These are sufficient but not necessary conditions.)</p>
<h1 id="reinforcement-learning">Reinforcement Learning</h1>
<h2 id="setup">Setup</h2>
<p>If we don't know $T$ and $R$ (i.e., we know only $S$ and $A$), then we get <strong>reinforcement learning</strong>. </p>
<p>Since we don't know $T$ and $R$, we have to "learn" them by having an agent interacting with the MDP. The general algorithm template is</p>
<blockquote>
<ul>
<li>For each iteration $t$:<blockquote>
<ul>
<li>Let the agent choose an action $a_t = \pi_\Mr{act}(s_{t-1})$ <em>somehow</em></li>
<li>Receive reward $r_t$ and new state $s_t$</li>
<li>Update parameters <em>somehow</em></li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<h2 id="on-policy-methods">On-Policy Methods</h2>
<p><strong><em>Goal</em></strong> Given simulations
$$s_0, a_1, r_1, s_1, a_2, r_2, s_2, \dots, a_n, r_n, s_n$$
from a specific policy $\pi$, estimate $Q_\pi(s,a)$.</p>
<p>One way is to compute the expected utility from the simulations:
$$u_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots$$
$$\hat Q_\pi(s,a) = \text{average of } u_t \text{ where } (s_{t-1}, a_t) = (s,a)$$
Equivalently, we can use the following algorithm to compute $\hat Q_\pi(s,a)$:</p>
<p><strong><em>Algorithm</em></strong> (<strong>Model-free Monte Carlo</strong>)</p>
<blockquote>
<ul>
<li>For each $(s,a,u)$ from the simulation:<blockquote>
<ul>
<li>Let $$\eta = \frac{1}{1 + \text{number of updates to }(s,a)}$$</li>
<li>Update using interpolation
$$\hat Q_\pi(s,a) \gets (1-\eta)\hat Q_\pi(s,a) + \eta u$$
or equivalently, update using "gradient" update
$$\hat Q_\pi(s,a) \gets \hat Q_\pi(s,a) - \eta\crab{\hat Q_\pi(s,a) - u}$$</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<p>In <strong>Model-free Monte Carlo</strong>, we have to compute $u$, which is expensive. As an alternative, <strong>SARSA</strong> approximates $u$ using the current reward plus the estimate of future reward.</p>
<p><strong><em>Algorithm</em></strong> (<strong>SARSA</strong>)</p>
<blockquote>
<ul>
<li>For each $(s,a,r,s',a')$ from the simulation:<blockquote>
<ul>
<li>Let $$\eta = \frac{1}{1 + \text{number of updates to }(s,a)}$$</li>
<li>Update using interpolation
$$\hat Q_\pi(s,a) \gets (1-\eta)\hat Q_\pi(s,a) + \eta\crab{r + \gamma \hat Q_\pi(s',a')}$$</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<h2 id="off-policy-methods">Off-Policy Methods</h2>
<p><strong><em>Goal</em></strong> Given simulations
$$s_0, a_1, r_1, s_1, a_2, r_2, s_2, \dots, a_n, r_n, s_n$$
from some policy, find the optimal policy and values.</p>
<p>There are 2 ways to do this. One way is <strong>model-based Monte Carlo</strong>, which recovers the MDP before computing $\pi_\Mr{opt}$ and $V_\Mr{opt}$. The $T$ and $R$ of the MDP can be approximated as
$$\hat T(s,a,s') = \frac{\Mr{count}(s,a,s')}{\Mr{count}(s,a)}$$
$$\hat R(s,a,s') = \text{average of } r \text{ in sequences } (s,a,r,s')$$
Then we can use value iteration or policy iteration to find the optimal policy and values.</p>
<p>The other way, <strong>Q-learning</strong>, approximates the optimal values directly without finding $T$ and $R$.</p>
<p><strong><em>Algorithm</em></strong> (<strong>Q-Learning</strong>)</p>
<blockquote>
<ul>
<li>For each $(s,a,r,s')$:<blockquote>
<ul>
<li>Update
$$\hat Q_\Mr{opt}(s,a) \gets (1-\eta)\hat Q_\Mr{opt}(s,a) + \eta\crab{r + \gamma \hat V_\Mr{opt}(s')}$$
where
$$\hat V_\Mr{opt}(s') = \max_{a'\in A(s')} \hat Q_\Mr{opt}(s',a')$$</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<p>Again, $V_\Mr{opt}$ and $\pi_\Mr{opt}$ can be recovered from $Q_\Mr{opt}$.</p>
<h2 id="exploration-vs-exploitation">Exploration vs. Exploitation</h2>
<p>Consider the policy $\pi_\Mr{act}$, which is used to generate simulations.</p>
<p>If we use $\pi_\Mr{act} = \pi$ to generate all simulations (<strong>exploitation</strong>), we may not be covering all states. To discover new states, we should make our agent perform random actions from time to time (<strong>exploration</strong>). But if we are too random, we may not get good states very often.</p>
<p>One common solution is <strong>epsilon-greedy policy</strong>:
$$\pi_\Mr{act}(s) = \cases{\argmax_{a\in A(s)}\hat Q_\Mr{opt}(s,a) &amp; \text{with probability } 1-\epsilon \\ \text{random action from } A(s) &amp; \text{with probability } \epsilon}$$</p>
<h2 id="function-approximation">Function Approximation</h2>
<p>To generalize better on large state spaces, we can make an assumption that
$$\hat Q_\Mr{opt}(s,a;w) = w\cdot \phi(s,a)$$
for some weight vector $w$ and feature function $\phi$. This is called <strong>function approximation</strong>. Q-learning becomes gradient descent:</p>
<blockquote>
<ul>
<li>For each $(s,a,r,s')$:<blockquote>
<ul>
<li>Update
$$w \gets w - \eta\crab{\hat Q_\Mr{opt}(s,a;w) - \nail{r + \gamma\hat V_\Mr{opt}(s')}}\phi(s,a)$$
where
$$\hat V_\Mr{opt}(s') = \max_{a'\in A(s')} \hat Q_\Mr{opt}(s',a')$$</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<p>This is basically gradient descent on $w$.</p>
<p>The function approximation can be replaced with any prediction function $f:S\times A \to \RR$ (e.g., neural network).</p>
<h1 id="reference">Reference</h1>
<ul>
<li>CS221 slides</li>
</ul>

      </div>
      <div id="content-time">Exported: 2016-07-13T02:05:22.238723</div>
    </div>
  </div>
</body>

</html>
